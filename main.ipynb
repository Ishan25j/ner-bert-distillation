{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from transformers import BertTokenizerFast, AutoModelForTokenClassification, DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\load.py:1429: FutureWarning: The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "conll_03 = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (14041, 5), 'validation': (3250, 5), 'test': (3453, 5)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_03.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_03['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_03['train'].features['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = conll_03['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_input = tokenizer(example_text['tokens'], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer_input['input_ids'])\n",
    "word_ids = tokenizer_input.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'eu',\n",
       " 'rejects',\n",
       " 'german',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'british',\n",
       " 'lamb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 11, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_text['ner_tags']), len(tokenizer_input['input_ids']), len(tokenizer_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "As shown above, the ner tags and tokenizer input doesn't have same length due to tokenizer using subwords token and cli tokens as addition.\n",
    "\n",
    "To solve this we have to refer the word_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "define new function `def tokenize_and_align_labels()` which performs two tasks\n",
    "\n",
    "1. set -100 as label for special tokens such as CLI token and the subwords we wish to mask during the training. Becase, PyTorch ignores the -100 index during the training time.\n",
    "2. mask the subword representations after the first subword\n",
    "\n",
    "Then we aligns the labels with tokens ids using the strategy we picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_aligns_labels(example, label_all_tokens=True):\n",
    "    tokenizer_input = tokenizer(example['tokens'], is_split_into_words=True, truncation=True)\n",
    "    labels = []\n",
    "    \n",
    "    for i, label in enumerate(example['ner_tags']):\n",
    "        word_ids = tokenizer_input.word_ids(batch_index=i) # word_ids() => List of mapped token indices to their actual word in the initial sentence.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenizer_input['labels'] = labels\n",
    "    return tokenizer_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = tokenize_and_aligns_labels(conll_03['train'][4:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}\n"
     ]
    }
   ],
   "source": [
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]___________________________________:-100\n",
      "germany_________________________________:5\n",
      "'_______________________________________:0\n",
      "s_______________________________________:0\n",
      "representative__________________________:0\n",
      "to______________________________________:0\n",
      "the_____________________________________:0\n",
      "european________________________________:3\n",
      "union___________________________________:4\n",
      "'_______________________________________:0\n",
      "s_______________________________________:0\n",
      "veterinary______________________________:0\n",
      "committee_______________________________:0\n",
      "werner__________________________________:1\n",
      "z_______________________________________:2\n",
      "##wing__________________________________:2\n",
      "##mann__________________________________:2\n",
      "said____________________________________:0\n",
      "on______________________________________:0\n",
      "wednesday_______________________________:0\n",
      "consumers_______________________________:0\n",
      "should__________________________________:0\n",
      "buy_____________________________________:0\n",
      "sheep___________________________________:0\n",
      "##me____________________________________:0\n",
      "##at____________________________________:0\n",
      "from____________________________________:0\n",
      "countries_______________________________:0\n",
      "other___________________________________:0\n",
      "than____________________________________:0\n",
      "britain_________________________________:5\n",
      "until___________________________________:0\n",
      "the_____________________________________:0\n",
      "scientific______________________________:0\n",
      "advice__________________________________:0\n",
      "was_____________________________________:0\n",
      "clearer_________________________________:0\n",
      "._______________________________________:0\n",
      "[SEP]___________________________________:-100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(q['input_ids'][0]), q['labels'][0]):\n",
    "    print(f'{token:_<40}:{label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = conll_03.map(tokenize_and_aligns_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "teacher = AutoModelForTokenClassification.from_pretrained('bert-base-uncased', num_labels=conll_03['train'].features['ner_tags'].feature.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_03['train'].features['ner_tags'].feature.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    'test-ner',\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer) \n",
    "# DataCollatorForTokenClassification => Collate function that is used to dynamically pad the inputs received by the model.\n",
    "# This is useful to pad the inputs to the maximum length of the batch with the maximum sequence length in the batch, which is not possible with the default data collate function provided by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_29960\\520486469.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric('seqeval')\n",
      "C:\\Users\\ishan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\load.py:752: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metric = datasets.load_metric('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = conll_03['train'].features['ner_tags'].feature.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for Mertics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_labels = [label_list[i] for i in example_text['ner_tags']]\n",
    "exp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MISC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=[exp_labels], references=[exp_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    \n",
    "    pred_logits, labels = eval_preds \n",
    "    \n",
    "    pred_logits = np.argmax(pred_logits, axis=2) \n",
    "    # the logits and the probabilities are in the same order,\n",
    "    # so we don’t need to apply the softmax\n",
    "    \n",
    "    # We remove all the values where the label is -100\n",
    "    predictions = [ \n",
    "        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100] \n",
    "        for prediction, label in zip(pred_logits, labels) \n",
    "    ] \n",
    "    \n",
    "    true_labels = [ \n",
    "      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100] \n",
    "       for prediction, label in zip(pred_logits, labels) \n",
    "    ] \n",
    "    \n",
    "    results = metric.compute(predictions=predictions, references=true_labels) \n",
    "    \n",
    "    return {\n",
    "        'precision': results['overall_precision'],\n",
    "        'recall': results['overall_recall'],\n",
    "        'f1': results['overall_f1'],\n",
    "        'accuracy': results['overall_accuracy']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    teacher,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "teacher.save_pretrained('ner-model')\n",
    "tokenizer.save_pretrained('ner-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config = json.load(open('ner-model/config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['id2label'] = id2label\n",
    "config['label2id'] = label2id\n",
    "\n",
    "json.dump(config, open('ner-model/config.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-ORG',\n",
       "  'score': 0.8851787,\n",
       "  'index': 1,\n",
       "  'word': 'hugging',\n",
       "  'start': 0,\n",
       "  'end': 7},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.88827014,\n",
       "  'index': 2,\n",
       "  'word': 'face',\n",
       "  'start': 8,\n",
       "  'end': 12},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.99571615,\n",
       "  'index': 5,\n",
       "  'word': 'french',\n",
       "  'start': 18,\n",
       "  'end': 24}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForTokenClassification, BertTokenizerFast\n",
    "\n",
    "model_finetuned = AutoModelForTokenClassification.from_pretrained('ner-model')\n",
    "\n",
    "tokenizer_finetuned = BertTokenizerFast.from_pretrained('ner-model')\n",
    "\n",
    "nlp = pipeline('ner', model=model_finetuned, tokenizer=tokenizer_finetuned)\n",
    "\n",
    "nlp('Hugging Face is a French company founded in 2016.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108898569"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get model number of parameters\n",
    "model_finetuned.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distill BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = conll_03['train'].features['ner_tags'].feature.names\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, BertTokenizerFast\n",
    "\n",
    "student_model_card = \"distilbert/distilbert-base-uncased\"\n",
    "student_model = AutoModelForTokenClassification.from_pretrained(student_model_card, num_labels=conll_03['train'].features['ner_tags'].feature.num_classes)\n",
    "student_tokenizer = BertTokenizerFast.from_pretrained(student_model_card)\n",
    "\n",
    "# Assuming `conll_03` is a dataset object and `id2label`, `label2id` are dictionaries defined earlier:\n",
    "student_tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    student_model_card, \n",
    "    num_labels=conll_03['train'].features['ner_tags'].feature.num_classes, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_aligns_labels_student(example, label_all_tokens=True):\n",
    "    tokenizer_input = student_tokenizer(example['tokens'], is_split_into_words=True, truncation=True)\n",
    "    labels = []\n",
    "    \n",
    "    for i, label in enumerate(example['ner_tags']):\n",
    "        word_ids = tokenizer_input.word_ids(batch_index=i) # word_ids() => List of mapped token indices to their actual word in the initial sentence.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenizer_input['labels'] = labels\n",
    "    return tokenizer_input\n",
    "\n",
    "tokenized_datasets_student = conll_03.map(tokenize_and_aligns_labels_student, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets_student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_student = DataCollatorForTokenClassification(student_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "# load teacher model\n",
    "teacher = AutoModelForTokenClassification.from_pretrained('ner-model')\n",
    "\n",
    "# load student model and teacher model on gpu\n",
    "\n",
    "teacher = teacher.to('cuda')\n",
    "student_model = student_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args_student = TrainingArguments(\n",
    "    'test-ner-student',\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "class KDTrainer(Trainer):\n",
    "    \n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, teacher=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        self.teacher = teacher\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \n",
    "        inputs = inputs.to(model.device)\n",
    "        \n",
    "        teacher_outputs = self.teacher(**inputs)\n",
    "        student_outputs = model(**inputs)\n",
    "        \n",
    "        # We don't need the logits, only the loss\n",
    "        teacher_logits = teacher_outputs.logits\n",
    "        student_logits = student_outputs.logits\n",
    "        \n",
    "        # print(teacher_logits.shape)\n",
    "        # print(student_logits.shape)\n",
    "\n",
    "        \n",
    "        # kd loss\n",
    "        loss_fct = nn.KLDivLoss(reduction='batchmean')\n",
    "        \n",
    "        loss_kd = loss_fct(\n",
    "            F.log_softmax(student_logits / self.temperature, dim=-1),\n",
    "            F.softmax(teacher_logits / self.temperature, dim=-1)\n",
    "        )\n",
    "        \n",
    "        # ce loss\n",
    "        loss_ce = student_outputs.loss\n",
    "        \n",
    "        # total loss\n",
    "        loss = (1 - self.alpha) * loss_ce + self.alpha * self.temperature ** 2 * loss_kd\n",
    "        \n",
    "        return (loss, student_outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\load.py:752: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = datasets.load_metric('seqeval')\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \n",
    "    pred_logits, labels = eval_preds \n",
    "    \n",
    "    pred_logits = np.argmax(pred_logits, axis=2) \n",
    "    # the logits and the probabilities are in the same order,\n",
    "    # so we don’t need to apply the softmax\n",
    "    \n",
    "    # We remove all the values where the label is -100\n",
    "    predictions = [ \n",
    "        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100] \n",
    "        for prediction, label in zip(pred_logits, labels) \n",
    "    ] \n",
    "    \n",
    "    true_labels = [ \n",
    "      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100] \n",
    "       for prediction, label in zip(pred_logits, labels) \n",
    "    ] \n",
    "    \n",
    "    results = metric.compute(predictions=predictions, references=true_labels) \n",
    "    \n",
    "    return {\n",
    "        'precision': results['overall_precision'],\n",
    "        'recall': results['overall_recall'],\n",
    "        'f1': results['overall_f1'],\n",
    "        'accuracy': results['overall_accuracy']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# trainer\n",
    "\n",
    "trainer_student = KDTrainer(\n",
    "    student_model,\n",
    "    args_student,\n",
    "    train_dataset=tokenized_datasets_student['train'],\n",
    "    eval_dataset=tokenized_datasets_student['validation'],\n",
    "    data_collator=data_collator_student,\n",
    "    tokenizer=student_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    teacher=teacher,\n",
    "    alpha=0.5,\n",
    "    temperature=2.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139eba54c36940019874d0a279385bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1756 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 120.0117, 'learning_rate': 1.988610478359909e-05, 'epoch': 0.01}\n",
      "{'loss': 64.7227, 'learning_rate': 1.977220956719818e-05, 'epoch': 0.02}\n",
      "{'loss': 37.7539, 'learning_rate': 1.9658314350797268e-05, 'epoch': 0.03}\n",
      "{'loss': 32.4821, 'learning_rate': 1.9544419134396357e-05, 'epoch': 0.05}\n",
      "{'loss': 32.2053, 'learning_rate': 1.9430523917995446e-05, 'epoch': 0.06}\n",
      "{'loss': 26.823, 'learning_rate': 1.9316628701594535e-05, 'epoch': 0.07}\n",
      "{'loss': 22.7437, 'learning_rate': 1.9202733485193623e-05, 'epoch': 0.08}\n",
      "{'loss': 22.0703, 'learning_rate': 1.9088838268792712e-05, 'epoch': 0.09}\n",
      "{'loss': 20.1889, 'learning_rate': 1.89749430523918e-05, 'epoch': 0.1}\n",
      "{'loss': 17.9789, 'learning_rate': 1.886104783599089e-05, 'epoch': 0.11}\n",
      "{'loss': 16.478, 'learning_rate': 1.874715261958998e-05, 'epoch': 0.13}\n",
      "{'loss': 15.728, 'learning_rate': 1.8633257403189068e-05, 'epoch': 0.14}\n",
      "{'loss': 15.5714, 'learning_rate': 1.8519362186788156e-05, 'epoch': 0.15}\n",
      "{'loss': 14.5941, 'learning_rate': 1.8405466970387245e-05, 'epoch': 0.16}\n",
      "{'loss': 14.469, 'learning_rate': 1.8291571753986334e-05, 'epoch': 0.17}\n",
      "{'loss': 13.7049, 'learning_rate': 1.8177676537585423e-05, 'epoch': 0.18}\n",
      "{'loss': 11.9455, 'learning_rate': 1.8063781321184512e-05, 'epoch': 0.19}\n",
      "{'loss': 12.418, 'learning_rate': 1.79498861047836e-05, 'epoch': 0.21}\n",
      "{'loss': 12.4287, 'learning_rate': 1.783599088838269e-05, 'epoch': 0.22}\n",
      "{'loss': 11.2235, 'learning_rate': 1.7722095671981778e-05, 'epoch': 0.23}\n",
      "{'loss': 11.821, 'learning_rate': 1.7608200455580867e-05, 'epoch': 0.24}\n",
      "{'loss': 11.7999, 'learning_rate': 1.7494305239179956e-05, 'epoch': 0.25}\n",
      "{'loss': 11.5451, 'learning_rate': 1.7380410022779045e-05, 'epoch': 0.26}\n",
      "{'loss': 10.3469, 'learning_rate': 1.7266514806378134e-05, 'epoch': 0.27}\n",
      "{'loss': 9.5376, 'learning_rate': 1.7152619589977222e-05, 'epoch': 0.28}\n",
      "{'loss': 9.994, 'learning_rate': 1.703872437357631e-05, 'epoch': 0.3}\n",
      "{'loss': 10.5591, 'learning_rate': 1.69248291571754e-05, 'epoch': 0.31}\n",
      "{'loss': 10.6796, 'learning_rate': 1.681093394077449e-05, 'epoch': 0.32}\n",
      "{'loss': 7.7404, 'learning_rate': 1.6697038724373578e-05, 'epoch': 0.33}\n",
      "{'loss': 8.4832, 'learning_rate': 1.6583143507972667e-05, 'epoch': 0.34}\n",
      "{'loss': 8.2071, 'learning_rate': 1.6469248291571755e-05, 'epoch': 0.35}\n",
      "{'loss': 7.4343, 'learning_rate': 1.6355353075170844e-05, 'epoch': 0.36}\n",
      "{'loss': 8.6147, 'learning_rate': 1.6241457858769933e-05, 'epoch': 0.38}\n",
      "{'loss': 8.7511, 'learning_rate': 1.6127562642369022e-05, 'epoch': 0.39}\n",
      "{'loss': 7.2706, 'learning_rate': 1.601366742596811e-05, 'epoch': 0.4}\n",
      "{'loss': 8.4021, 'learning_rate': 1.58997722095672e-05, 'epoch': 0.41}\n",
      "{'loss': 7.459, 'learning_rate': 1.578587699316629e-05, 'epoch': 0.42}\n",
      "{'loss': 7.0822, 'learning_rate': 1.5671981776765377e-05, 'epoch': 0.43}\n",
      "{'loss': 6.8229, 'learning_rate': 1.5558086560364466e-05, 'epoch': 0.44}\n",
      "{'loss': 6.7409, 'learning_rate': 1.5444191343963555e-05, 'epoch': 0.46}\n",
      "{'loss': 7.1796, 'learning_rate': 1.5330296127562644e-05, 'epoch': 0.47}\n",
      "{'loss': 6.6433, 'learning_rate': 1.5216400911161733e-05, 'epoch': 0.48}\n",
      "{'loss': 6.6069, 'learning_rate': 1.5102505694760821e-05, 'epoch': 0.49}\n",
      "{'loss': 7.0063, 'learning_rate': 1.498861047835991e-05, 'epoch': 0.5}\n",
      "{'loss': 5.9824, 'learning_rate': 1.4874715261958999e-05, 'epoch': 0.51}\n",
      "{'loss': 6.214, 'learning_rate': 1.4760820045558088e-05, 'epoch': 0.52}\n",
      "{'loss': 7.2292, 'learning_rate': 1.4646924829157177e-05, 'epoch': 0.54}\n",
      "{'loss': 6.3206, 'learning_rate': 1.4533029612756266e-05, 'epoch': 0.55}\n",
      "{'loss': 5.3866, 'learning_rate': 1.4419134396355354e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory test-ner-student\\checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2463, 'learning_rate': 1.4305239179954442e-05, 'epoch': 0.57}\n",
      "{'loss': 6.249, 'learning_rate': 1.4191343963553532e-05, 'epoch': 0.58}\n",
      "{'loss': 5.7947, 'learning_rate': 1.407744874715262e-05, 'epoch': 0.59}\n",
      "{'loss': 5.1118, 'learning_rate': 1.396355353075171e-05, 'epoch': 0.6}\n",
      "{'loss': 5.6704, 'learning_rate': 1.3849658314350799e-05, 'epoch': 0.62}\n",
      "{'loss': 5.1358, 'learning_rate': 1.3735763097949887e-05, 'epoch': 0.63}\n",
      "{'loss': 6.0096, 'learning_rate': 1.3621867881548976e-05, 'epoch': 0.64}\n",
      "{'loss': 6.3806, 'learning_rate': 1.3507972665148065e-05, 'epoch': 0.65}\n",
      "{'loss': 6.1514, 'learning_rate': 1.3394077448747154e-05, 'epoch': 0.66}\n",
      "{'loss': 5.8057, 'learning_rate': 1.3280182232346241e-05, 'epoch': 0.67}\n",
      "{'loss': 6.1939, 'learning_rate': 1.3166287015945332e-05, 'epoch': 0.68}\n",
      "{'loss': 5.5551, 'learning_rate': 1.3052391799544419e-05, 'epoch': 0.69}\n",
      "{'loss': 5.0283, 'learning_rate': 1.293849658314351e-05, 'epoch': 0.71}\n",
      "{'loss': 4.2156, 'learning_rate': 1.2824601366742598e-05, 'epoch': 0.72}\n",
      "{'loss': 5.76, 'learning_rate': 1.2710706150341687e-05, 'epoch': 0.73}\n",
      "{'loss': 6.8668, 'learning_rate': 1.2596810933940776e-05, 'epoch': 0.74}\n",
      "{'loss': 5.054, 'learning_rate': 1.2482915717539865e-05, 'epoch': 0.75}\n",
      "{'loss': 5.0813, 'learning_rate': 1.2369020501138953e-05, 'epoch': 0.76}\n",
      "{'loss': 5.5231, 'learning_rate': 1.2255125284738042e-05, 'epoch': 0.77}\n",
      "{'loss': 5.4218, 'learning_rate': 1.2141230068337131e-05, 'epoch': 0.79}\n",
      "{'loss': 5.3758, 'learning_rate': 1.2027334851936218e-05, 'epoch': 0.8}\n",
      "{'loss': 6.0225, 'learning_rate': 1.1913439635535309e-05, 'epoch': 0.81}\n",
      "{'loss': 5.0602, 'learning_rate': 1.1799544419134396e-05, 'epoch': 0.82}\n",
      "{'loss': 5.144, 'learning_rate': 1.1685649202733486e-05, 'epoch': 0.83}\n",
      "{'loss': 5.1727, 'learning_rate': 1.1571753986332575e-05, 'epoch': 0.84}\n",
      "{'loss': 6.3626, 'learning_rate': 1.1457858769931664e-05, 'epoch': 0.85}\n",
      "{'loss': 4.3094, 'learning_rate': 1.1343963553530753e-05, 'epoch': 0.87}\n",
      "{'loss': 5.1328, 'learning_rate': 1.1230068337129842e-05, 'epoch': 0.88}\n",
      "{'loss': 4.5917, 'learning_rate': 1.111617312072893e-05, 'epoch': 0.89}\n",
      "{'loss': 4.1478, 'learning_rate': 1.1002277904328018e-05, 'epoch': 0.9}\n",
      "{'loss': 3.9503, 'learning_rate': 1.0888382687927108e-05, 'epoch': 0.91}\n",
      "{'loss': 5.2104, 'learning_rate': 1.0774487471526195e-05, 'epoch': 0.92}\n",
      "{'loss': 4.9876, 'learning_rate': 1.0660592255125286e-05, 'epoch': 0.93}\n",
      "{'loss': 4.8094, 'learning_rate': 1.0546697038724373e-05, 'epoch': 0.95}\n",
      "{'loss': 4.2665, 'learning_rate': 1.0432801822323464e-05, 'epoch': 0.96}\n",
      "{'loss': 4.075, 'learning_rate': 1.0318906605922552e-05, 'epoch': 0.97}\n",
      "{'loss': 4.7974, 'learning_rate': 1.0205011389521641e-05, 'epoch': 0.98}\n",
      "{'loss': 4.6617, 'learning_rate': 1.009111617312073e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb09aacdca2e44cf96e0c8d87428f838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.05456280708313, 'eval_precision': 0.8764436696448028, 'eval_recall': 0.8998769437297237, 'eval_f1': 0.888005740464757, 'eval_accuracy': 0.9758685878596279, 'eval_runtime': 6.0873, 'eval_samples_per_second': 533.901, 'eval_steps_per_second': 33.513, 'epoch': 1.0}\n",
      "{'loss': 4.5301, 'learning_rate': 9.977220956719819e-06, 'epoch': 1.0}\n",
      "{'loss': 4.1304, 'learning_rate': 9.863325740318908e-06, 'epoch': 1.01}\n",
      "{'loss': 5.2209, 'learning_rate': 9.749430523917997e-06, 'epoch': 1.03}\n",
      "{'loss': 4.5231, 'learning_rate': 9.635535307517085e-06, 'epoch': 1.04}\n",
      "{'loss': 3.8869, 'learning_rate': 9.521640091116174e-06, 'epoch': 1.05}\n",
      "{'loss': 4.0269, 'learning_rate': 9.407744874715261e-06, 'epoch': 1.06}\n",
      "{'loss': 3.8067, 'learning_rate': 9.293849658314352e-06, 'epoch': 1.07}\n",
      "{'loss': 3.3414, 'learning_rate': 9.17995444191344e-06, 'epoch': 1.08}\n",
      "{'loss': 4.4124, 'learning_rate': 9.06605922551253e-06, 'epoch': 1.09}\n",
      "{'loss': 3.7504, 'learning_rate': 8.952164009111618e-06, 'epoch': 1.1}\n",
      "{'loss': 4.377, 'learning_rate': 8.838268792710707e-06, 'epoch': 1.12}\n",
      "{'loss': 4.0118, 'learning_rate': 8.724373576309796e-06, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory test-ner-student\\checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7391, 'learning_rate': 8.610478359908885e-06, 'epoch': 1.14}\n",
      "{'loss': 4.3186, 'learning_rate': 8.496583143507974e-06, 'epoch': 1.15}\n",
      "{'loss': 3.2366, 'learning_rate': 8.382687927107063e-06, 'epoch': 1.16}\n",
      "{'loss': 4.7049, 'learning_rate': 8.26879271070615e-06, 'epoch': 1.17}\n",
      "{'loss': 3.3936, 'learning_rate': 8.15489749430524e-06, 'epoch': 1.18}\n",
      "{'loss': 4.6457, 'learning_rate': 8.041002277904329e-06, 'epoch': 1.2}\n",
      "{'loss': 4.7592, 'learning_rate': 7.927107061503418e-06, 'epoch': 1.21}\n",
      "{'loss': 4.2475, 'learning_rate': 7.813211845102507e-06, 'epoch': 1.22}\n",
      "{'loss': 4.1489, 'learning_rate': 7.699316628701596e-06, 'epoch': 1.23}\n",
      "{'loss': 4.4393, 'learning_rate': 7.585421412300684e-06, 'epoch': 1.24}\n",
      "{'loss': 4.2335, 'learning_rate': 7.471526195899773e-06, 'epoch': 1.25}\n",
      "{'loss': 3.4916, 'learning_rate': 7.357630979498862e-06, 'epoch': 1.26}\n",
      "{'loss': 4.1418, 'learning_rate': 7.243735763097951e-06, 'epoch': 1.28}\n",
      "{'loss': 3.2802, 'learning_rate': 7.129840546697039e-06, 'epoch': 1.29}\n",
      "{'loss': 3.6427, 'learning_rate': 7.015945330296128e-06, 'epoch': 1.3}\n",
      "{'loss': 4.0104, 'learning_rate': 6.9020501138952166e-06, 'epoch': 1.31}\n",
      "{'loss': 3.8504, 'learning_rate': 6.788154897494305e-06, 'epoch': 1.32}\n",
      "{'loss': 4.0629, 'learning_rate': 6.674259681093394e-06, 'epoch': 1.33}\n",
      "{'loss': 3.1864, 'learning_rate': 6.560364464692484e-06, 'epoch': 1.34}\n",
      "{'loss': 3.2363, 'learning_rate': 6.446469248291573e-06, 'epoch': 1.36}\n",
      "{'loss': 4.2123, 'learning_rate': 6.3325740318906616e-06, 'epoch': 1.37}\n",
      "{'loss': 4.6676, 'learning_rate': 6.21867881548975e-06, 'epoch': 1.38}\n",
      "{'loss': 3.6233, 'learning_rate': 6.104783599088838e-06, 'epoch': 1.39}\n",
      "{'loss': 4.288, 'learning_rate': 5.990888382687927e-06, 'epoch': 1.4}\n",
      "{'loss': 3.7584, 'learning_rate': 5.876993166287016e-06, 'epoch': 1.41}\n",
      "{'loss': 3.7025, 'learning_rate': 5.763097949886105e-06, 'epoch': 1.42}\n",
      "{'loss': 4.2495, 'learning_rate': 5.649202733485194e-06, 'epoch': 1.44}\n",
      "{'loss': 3.0817, 'learning_rate': 5.5353075170842826e-06, 'epoch': 1.45}\n",
      "{'loss': 3.1517, 'learning_rate': 5.421412300683372e-06, 'epoch': 1.46}\n",
      "{'loss': 3.7001, 'learning_rate': 5.307517084282461e-06, 'epoch': 1.47}\n",
      "{'loss': 3.1376, 'learning_rate': 5.19362186788155e-06, 'epoch': 1.48}\n",
      "{'loss': 2.9745, 'learning_rate': 5.079726651480639e-06, 'epoch': 1.49}\n",
      "{'loss': 4.2347, 'learning_rate': 4.965831435079727e-06, 'epoch': 1.5}\n",
      "{'loss': 3.3995, 'learning_rate': 4.851936218678816e-06, 'epoch': 1.51}\n",
      "{'loss': 3.6562, 'learning_rate': 4.738041002277905e-06, 'epoch': 1.53}\n",
      "{'loss': 3.688, 'learning_rate': 4.624145785876993e-06, 'epoch': 1.54}\n",
      "{'loss': 3.4487, 'learning_rate': 4.510250569476082e-06, 'epoch': 1.55}\n",
      "{'loss': 2.9225, 'learning_rate': 4.396355353075171e-06, 'epoch': 1.56}\n",
      "{'loss': 4.3376, 'learning_rate': 4.2824601366742606e-06, 'epoch': 1.57}\n",
      "{'loss': 3.9863, 'learning_rate': 4.168564920273349e-06, 'epoch': 1.58}\n",
      "{'loss': 3.6686, 'learning_rate': 4.054669703872437e-06, 'epoch': 1.59}\n",
      "{'loss': 3.7531, 'learning_rate': 3.940774487471526e-06, 'epoch': 1.61}\n",
      "{'loss': 3.6869, 'learning_rate': 3.826879271070615e-06, 'epoch': 1.62}\n",
      "{'loss': 3.221, 'learning_rate': 3.7129840546697043e-06, 'epoch': 1.63}\n",
      "{'loss': 4.1431, 'learning_rate': 3.5990888382687927e-06, 'epoch': 1.64}\n",
      "{'loss': 3.6908, 'learning_rate': 3.4851936218678815e-06, 'epoch': 1.65}\n",
      "{'loss': 2.8327, 'learning_rate': 3.371298405466971e-06, 'epoch': 1.66}\n",
      "{'loss': 4.3619, 'learning_rate': 3.2574031890660596e-06, 'epoch': 1.67}\n",
      "{'loss': 3.3447, 'learning_rate': 3.1435079726651485e-06, 'epoch': 1.69}\n",
      "{'loss': 3.4489, 'learning_rate': 3.029612756264237e-06, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory test-ner-student\\checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.408, 'learning_rate': 2.9157175398633257e-06, 'epoch': 1.71}\n",
      "{'loss': 3.2871, 'learning_rate': 2.801822323462415e-06, 'epoch': 1.72}\n",
      "{'loss': 3.5728, 'learning_rate': 2.687927107061504e-06, 'epoch': 1.73}\n",
      "{'loss': 3.6608, 'learning_rate': 2.5740318906605926e-06, 'epoch': 1.74}\n",
      "{'loss': 3.0918, 'learning_rate': 2.4601366742596815e-06, 'epoch': 1.75}\n",
      "{'loss': 3.3896, 'learning_rate': 2.34624145785877e-06, 'epoch': 1.77}\n",
      "{'loss': 2.8706, 'learning_rate': 2.232346241457859e-06, 'epoch': 1.78}\n",
      "{'loss': 3.4026, 'learning_rate': 2.118451025056948e-06, 'epoch': 1.79}\n",
      "{'loss': 3.6588, 'learning_rate': 2.0045558086560364e-06, 'epoch': 1.8}\n",
      "{'loss': 4.1256, 'learning_rate': 1.8906605922551254e-06, 'epoch': 1.81}\n",
      "{'loss': 3.5231, 'learning_rate': 1.7767653758542143e-06, 'epoch': 1.82}\n",
      "{'loss': 3.3512, 'learning_rate': 1.662870159453303e-06, 'epoch': 1.83}\n",
      "{'loss': 3.4139, 'learning_rate': 1.5489749430523921e-06, 'epoch': 1.85}\n",
      "{'loss': 3.5857, 'learning_rate': 1.4350797266514807e-06, 'epoch': 1.86}\n",
      "{'loss': 3.0586, 'learning_rate': 1.3211845102505696e-06, 'epoch': 1.87}\n",
      "{'loss': 2.8306, 'learning_rate': 1.2072892938496584e-06, 'epoch': 1.88}\n",
      "{'loss': 3.1758, 'learning_rate': 1.0933940774487472e-06, 'epoch': 1.89}\n",
      "{'loss': 2.8086, 'learning_rate': 9.79498861047836e-07, 'epoch': 1.9}\n",
      "{'loss': 3.2522, 'learning_rate': 8.656036446469249e-07, 'epoch': 1.91}\n",
      "{'loss': 2.9114, 'learning_rate': 7.517084282460136e-07, 'epoch': 1.92}\n",
      "{'loss': 3.7285, 'learning_rate': 6.378132118451026e-07, 'epoch': 1.94}\n",
      "{'loss': 3.9799, 'learning_rate': 5.239179954441914e-07, 'epoch': 1.95}\n",
      "{'loss': 3.6373, 'learning_rate': 4.1002277904328024e-07, 'epoch': 1.96}\n",
      "{'loss': 3.7521, 'learning_rate': 2.961275626423691e-07, 'epoch': 1.97}\n",
      "{'loss': 3.2535, 'learning_rate': 1.8223234624145786e-07, 'epoch': 1.98}\n",
      "{'loss': 3.3528, 'learning_rate': 6.83371298405467e-08, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940254d40f7b45f882397a29e10f1ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4997339248657227, 'eval_precision': 0.9012549537648613, 'eval_recall': 0.9158742588656449, 'eval_f1': 0.9085057981468124, 'eval_accuracy': 0.9793794779735333, 'eval_runtime': 10.1415, 'eval_samples_per_second': 320.465, 'eval_steps_per_second': 20.115, 'epoch': 2.0}\n",
      "{'train_runtime': 210.3598, 'train_samples_per_second': 133.495, 'train_steps_per_second': 8.348, 'train_loss': 7.4272418000431975, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1756, training_loss=7.4272418000431975, metrics={'train_runtime': 210.3598, 'train_samples_per_second': 133.495, 'train_steps_per_second': 8.348, 'train_loss': 7.4272418000431975, 'epoch': 2.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_student.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# load student model\n",
    "student_model.save_pretrained('ner-student')\n",
    "student_tokenizer.save_pretrained('ner-student')\n",
    "\n",
    "config = json.load(open('ner-student/config.json'))\n",
    "\n",
    "config['id2label'] = id2label\n",
    "config['label2id'] = label2id\n",
    "\n",
    "\n",
    "json.dump(config, open('ner-student/config.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-ORG',\n",
       "  'score': 0.5237874388694763,\n",
       "  'index': 1,\n",
       "  'word': 'hugging',\n",
       "  'start': 17662,\n",
       "  'end': 17669},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.9951995611190796,\n",
       "  'index': 5,\n",
       "  'word': 'french',\n",
       "  'start': 2413,\n",
       "  'end': 2419},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.24349796772003174,\n",
       "  'index': 11,\n",
       "  'word': '[SEP]',\n",
       "  'start': 102,\n",
       "  'end': 107}]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, BertTokenizerFast, pipeline\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load student model and tokenizer\n",
    "student_model = AutoModelForTokenClassification.from_pretrained('ner-student')\n",
    "student_tokenizer = BertTokenizerFast.from_pretrained('ner-student')\n",
    "\n",
    "# Define label mapping (id2label)\n",
    "id2label = student_model.config.id2label\n",
    "\n",
    "# Modify pipeline to remove token_type_ids and get human-readable output\n",
    "def custom_pipeline(text):\n",
    "    # Get the inputs from the tokenizer\n",
    "    inputs = student_tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    \n",
    "    # Remove 'token_type_ids' if present (as DistilBERT doesn't use them)\n",
    "    if 'token_type_ids' in inputs:\n",
    "        inputs.pop('token_type_ids')\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        output = student_model(**inputs)\n",
    "\n",
    "    logits = output.logits\n",
    "    probabilities = F.softmax(logits, dim=-1)  # Get probabilities using softmax\n",
    "    predictions = torch.argmax(probabilities, dim=-1).squeeze()  # Get the index of the highest score\n",
    "    \n",
    "    # Convert predictions to label and score\n",
    "    results = []\n",
    "    tokens = student_tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze())\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.startswith('##'):  # Skip subword tokens to avoid broken token entities\n",
    "            continue\n",
    "        \n",
    "        label_id = predictions[i].item()\n",
    "        label = id2label[label_id]\n",
    "        score = probabilities[0][i][label_id].item()  # Get the score for the predicted label\n",
    "        \n",
    "        # Add the entity only if it is not 'O' (no entity)\n",
    "        if label != 'O':\n",
    "            results.append({\n",
    "                'entity': label,\n",
    "                'score': score,\n",
    "                'index': i,\n",
    "                'word': token,\n",
    "                'start': inputs['input_ids'][0][i].item(),\n",
    "                'end': inputs['input_ids'][0][i].item() + len(token)  # Estimate start/end\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the custom pipeline\n",
    "text = \"Hugging Face is a French company founded in 2016.\"\n",
    "result = custom_pipeline(text)\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare BERT and DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Student Parameters: 66.370M\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "parameters = filter(lambda p: p.requires_grad, student_model.parameters())\n",
    "parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
    "print('Trainable Student Parameters: %.3fM' % parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Teacher Parameters: 108.899M\n"
     ]
    }
   ],
   "source": [
    "# teacher model\n",
    "parameters = filter(lambda p: p.requires_grad, teacher.parameters())\n",
    "parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
    "print('Trainable Teacher Parameters: %.3fM' % parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def step(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self.step_and_update_lr()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenClassifierOutput(loss=tensor(2.7006, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[ 4.8585e-01,  1.1119e-01, -1.1505e+00,  8.9645e-01, -1.7438e-01,\n",
      "          -3.7746e-01, -3.5475e-01, -1.9035e-01, -4.6150e-01],\n",
      "         [ 3.0458e-01,  5.0577e-01, -6.0738e-01,  9.8700e-02, -1.0883e-01,\n",
      "           3.2272e-01, -2.8224e-01, -5.0890e-01, -7.9754e-02],\n",
      "         [ 8.1097e-01, -6.8989e-01,  6.6812e-01,  1.7677e-01, -4.6742e-01,\n",
      "          -7.3528e-01, -5.8350e-02,  3.5619e-01,  1.6835e-01],\n",
      "         [-4.4839e-01,  7.8674e-01, -2.5306e-01,  2.5320e-01, -8.9091e-01,\n",
      "           5.2139e-01, -7.9422e-01, -1.8371e-01, -1.5040e-01],\n",
      "         [-1.2329e+00,  8.9540e-03, -3.4395e-01,  1.0705e-01, -2.3584e-01,\n",
      "           3.2006e-01,  4.3780e-01, -8.5086e-01,  9.9013e-01],\n",
      "         [ 1.0983e-01,  2.5225e-01, -1.5263e-01,  9.9148e-01, -1.0004e-01,\n",
      "          -3.8962e-01, -1.0051e-01,  7.9216e-01,  1.6516e-02],\n",
      "         [ 7.0605e-02,  3.3741e-01, -3.9578e-01, -4.4323e-02, -2.1545e-01,\n",
      "          -1.1611e-01, -8.5383e-02, -2.6896e-01,  2.2294e-01],\n",
      "         [ 1.3870e-01, -2.1214e-01,  4.1390e-01,  2.4000e-01, -4.1539e-01,\n",
      "           9.8982e-01,  3.6555e-01, -1.4596e+00,  4.6082e-01],\n",
      "         [-1.0545e+00,  1.5781e-02,  5.4538e-01,  4.1812e-01,  9.4062e-01,\n",
      "           2.0173e-02,  7.2413e-02,  7.8085e-01,  1.8891e-01],\n",
      "         [-6.0614e-01,  6.1439e-01, -9.5001e-02,  1.5695e-01,  1.2991e-01,\n",
      "          -2.8111e-01,  1.5748e-01,  1.3751e+00,  5.9134e-01],\n",
      "         [ 1.4123e-01, -7.3852e-01, -9.0576e-03,  1.1434e-01,  2.2535e-01,\n",
      "          -2.5945e-01, -1.0761e-01, -8.7260e-01,  4.9943e-01]],\n",
      "\n",
      "        [[ 1.9649e-01,  1.5232e-01, -1.0540e+00,  7.7805e-01, -5.0613e-02,\n",
      "          -6.8617e-01, -4.5174e-01, -4.6303e-02, -7.5607e-01],\n",
      "         [ 6.8808e-01,  1.6045e-01, -1.4606e-03, -5.7233e-02, -2.3646e-01,\n",
      "           5.1030e-01,  8.8480e-02, -3.7165e-01, -1.1947e-01],\n",
      "         [ 7.9606e-01, -4.2241e-01,  9.5820e-01,  3.3221e-01, -2.3545e-01,\n",
      "          -7.3590e-01, -5.8885e-02,  6.7900e-01,  9.8740e-02],\n",
      "         [-1.8127e-01,  2.4272e-01, -3.7982e-01, -9.7354e-02, -6.4505e-01,\n",
      "           4.6116e-01, -8.1902e-01, -4.8859e-01, -4.0293e-01],\n",
      "         [-7.0831e-01,  8.6193e-01, -6.4966e-01,  1.8405e-01, -6.0145e-01,\n",
      "           6.2719e-01,  5.1699e-02, -7.4646e-01,  9.7586e-01],\n",
      "         [ 5.9754e-01,  1.9741e-01,  5.5426e-02,  6.2600e-01, -2.7703e-01,\n",
      "           2.0313e-02,  2.2755e-01,  8.4036e-01,  1.0076e-01],\n",
      "         [ 3.4745e-01,  3.0329e-01, -2.6611e-01, -2.4709e-02, -2.5425e-01,\n",
      "          -7.1504e-02, -2.2504e-01, -1.6902e-01,  2.9359e-01],\n",
      "         [-1.2866e-02,  6.6199e-02,  4.8530e-01, -2.0337e-01, -4.5095e-01,\n",
      "           1.1625e+00,  1.4064e-01, -1.0003e+00,  1.2977e-01],\n",
      "         [-8.4516e-01, -6.7400e-02,  2.9040e-01,  4.2726e-01,  9.5352e-01,\n",
      "           1.8500e-01,  4.2270e-01,  1.4833e+00, -1.2628e-01],\n",
      "         [-9.0112e-01,  1.1693e-01,  5.7744e-01,  4.6469e-01, -4.5793e-04,\n",
      "          -4.7659e-01, -5.6354e-02,  1.0989e+00,  8.2294e-01],\n",
      "         [ 5.0157e-01, -7.7707e-01, -9.5417e-02,  1.1282e-01,  5.4570e-01,\n",
      "          -5.4044e-01,  1.5712e-02, -7.4682e-01,  9.0800e-02]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.zeros(features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.dropout(F.relu(self.linear_1(x))))\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure that the scaling tensor is on the same device as x\n",
    "        device = x.device\n",
    "        multiply_by_sqrt_d_model = torch.sqrt(torch.tensor(self.d_model, dtype=torch.float, device=device))\n",
    "        return self.embedding(x) * multiply_by_sqrt_d_model\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, features: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, attn_dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.attn_dropout = attn_dropout\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        n_samples, n_patches, dim = x.shape\n",
    "\n",
    "        if dim != self.embed_dim:\n",
    "            raise ValueError(f\"Input embedding dimension ({dim}) doesn't match model embedding dimension ({self.embed_dim})\")\n",
    "\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.reshape(n_samples, n_patches, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        k_t = k.transpose(-2, -1)\n",
    "        attn = (q @ k_t) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask.unsqueeze(1).unsqueeze(2) == 0, float(\"-inf\"))\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        weighted_avg = attn @ v\n",
    "\n",
    "        weighted_avg = weighted_avg.transpose(1, 2)\n",
    "        weighted_avg = weighted_avg.flatten(2)\n",
    "\n",
    "        x = self.fc_out(weighted_avg)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, features: int, num_layers: int, num_heads: int, d_ff: int, dropout: float, max_len: int, vocab_size: int, num_labels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.input_embeddings = InputEmbeddings(features, vocab_size)\n",
    "        self.positional_encoding = PositionalEncoding(features, max_len, dropout)\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            EncoderBlock(features, MultiHeadAttention(features, num_heads), FeedForwardBlock(features, d_ff, dropout), dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.encoder = Encoder(features, self.encoder_blocks)\n",
    "        self.classifier = nn.Linear(features, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None, device=\"cuda\"):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        \n",
    "        if labels is not None:\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "        x = self.input_embeddings(input_ids)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.encoder(x, attention_mask)\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # Only consider the active parts of the loss\n",
    "            active_loss = attention_mask.view(-1) == 1\n",
    "            active_logits = logits.view(-1, logits.size(-1))\n",
    "            active_labels = torch.where(\n",
    "                active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "            )\n",
    "            loss = loss_fct(active_logits, active_labels)\n",
    "\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits)\n",
    "\n",
    "# Example usage\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = BERT(features=768, num_layers=8, num_heads=8, d_ff=2048, dropout=0.1, max_len=512, vocab_size=30522, num_labels=9).to(device)\n",
    "\n",
    "inputs = {\n",
    "    'input_ids': torch.tensor([[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], [101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102]]).to(device),\n",
    "    'attention_mask': torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]).to(device),\n",
    "    'token_type_ids': torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]).to(device),\n",
    "    'labels': torch.tensor([[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100], [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100]]).to(device)\n",
    "}\n",
    "\n",
    "output = model(**inputs)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Teacher Parameters: 108.899M\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "parameters = filter(lambda p: p.requires_grad, model_finetuned.parameters())\n",
    "parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
    "print('Trainable Teacher Parameters: %.3fM' % parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Student Parameters: 67.561M\n"
     ]
    }
   ],
   "source": [
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
    "print('Trainable Student Parameters: %.3fM' % parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trainer for knowledge distillation of Student model with Teacher model\n",
    "class DistillationTrainer():\n",
    "    \n",
    "    def __init__(self, teacher, student, train_dataloader, val_dataloader, optimizer, criterion, temperature, device, scheduler=None):\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "        self.temperature = temperature\n",
    "        self.alpha = 0.5\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        self.teacher.eval()\n",
    "        self.student.train()\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = 0\n",
    "            for i, batch in enumerate(self.train_dataloader):\n",
    "                \n",
    "                \n",
    "                filtered_inputs = {\n",
    "                    'input_ids': torch.tensor(batch['input_ids']).unsqueeze(0),  # unsqueeze to add batch dimension\n",
    "                    'attention_mask': torch.tensor(batch['attention_mask']).unsqueeze(0),\n",
    "                    'token_type_ids': torch.tensor(batch.get('token_type_ids', [])).unsqueeze(0) if 'token_type_ids' in batch else None,\n",
    "                    'labels': torch.tensor(batch.get('labels', [])).unsqueeze(0) if 'labels' in batch else None\n",
    "                }\n",
    "                \n",
    "                filtered_inputs = {k: v for k, v in filtered_inputs.items() if v is not None}\n",
    "                \n",
    "                # print(filtered_inputs)\n",
    "                self.optimizer.zero_grad()\n",
    "                teacher_output = self.teacher(**filtered_inputs)\n",
    "                student_output = self.student(**filtered_inputs)\n",
    "                \n",
    "                loss_ce = student_output.loss\n",
    "                \n",
    "                logits_student = student_output.logits\n",
    "                logits_teacher = teacher_output.logits\n",
    "                \n",
    "                loss_kd = self.temperature ** 2 * loss_fct(\n",
    "                    F.log_softmax(logits_student / self.temperature, dim=-1),\n",
    "                    F.softmax(logits_teacher / self.temperature, dim=-1)\n",
    "                )\n",
    "                \n",
    "                loss = self.alpha * loss_ce + (1 - self.alpha) * loss_kd\n",
    "\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step_and_update_lr()\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                if i % 10 == 0:\n",
    "                    print(f'Epoch: {epoch}, Iteration: {i}, Loss: {train_loss / (i + 1)}')\n",
    "                    \n",
    "            val_loss = self.evaluate()\n",
    "            print(f'Epoch: {epoch}, Validation Loss: {val_loss}')\n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step(val_loss)\n",
    "                \n",
    "            # save checkpoint after certain epochs\n",
    "            if epoch % 5 == 0:\n",
    "                torch.save(self.student.state_dict(), f'student_epoch_{epoch}.pth')\n",
    "                \n",
    "                \n",
    "                \n",
    "    def evaluate(self):\n",
    "        self.student.eval()\n",
    "        val_loss = 0\n",
    "        for i, batch in enumerate(self.val_dataloader):\n",
    "            filtered_inputs = {\n",
    "                'input_ids': torch.tensor(batch['input_ids']).to(self.device),\n",
    "                'attention_mask': torch.tensor(batch['attention_mask']).to(self.device),\n",
    "                'token_type_ids': torch.tensor(batch.get('token_type_ids', [])).to(self.device) if 'token_type_ids' in batch else None,\n",
    "                'labels': torch.tensor(batch.get('labels', [])).to(self.device) if 'labels' in batch else None\n",
    "            }\n",
    "\n",
    "            filtered_inputs = {k: v for k, v in filtered_inputs.items() if v is not None}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                student_output = self.student(**filtered_inputs)\n",
    "                val_loss += self.criterion(student_output.logits.view(-1, student_output.logits.size(-1)), filtered_inputs['labels'].view(-1)).item()\n",
    "                \n",
    "        return val_loss / len(self.val_dataloader)\n",
    "    \n",
    "    \n",
    "\n",
    "# Define the optimizer and scheduler\n",
    "optimizer = ScheduledOptim(torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9), 768, 2000)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = DistillationTrainer(model_finetuned, model, tokenized_datasets['train'], tokenized_datasets['validation'], optimizer, loss_fct, 2, 'cuda')\n",
    "\n",
    "# Train the student model\n",
    "# trainer.train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer():\n",
    "    \n",
    "    def __init__(self, teacher, student, train_dataloader, val_dataloader, optimizer, criterion, temperature, device, scheduler=None):\n",
    "        self.teacher = teacher.to(device)\n",
    "        self.student = student.to(device)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "        self.temperature = temperature\n",
    "        self.alpha = 0.5\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        self.teacher.eval()\n",
    "        # self.student.train()\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = 0\n",
    "            for i, batch in enumerate(self.train_dataloader):\n",
    "                \n",
    "                filtered_inputs = {\n",
    "                    'input_ids': batch['input_ids'].to(self.device),  \n",
    "                    'attention_mask': batch['attention_mask'].to(self.device),\n",
    "                    'token_type_ids': batch.get('token_type_ids', torch.tensor([])).to(self.device) if 'token_type_ids' in batch else None,\n",
    "                    'labels': batch.get('labels', torch.tensor([])).to(self.device) if 'labels' in batch else None\n",
    "                }\n",
    "                \n",
    "                \n",
    "                # Ensure that all necessary inputs are provided\n",
    "                filtered_inputs = {k: v for k, v in filtered_inputs.items() if v is not None}\n",
    "                \n",
    "                # set fileterd inputs to the device\n",
    "                filtered_inputs = {k: v.to(self.device) for k, v in filtered_inputs.items()}\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                teacher_output = self.teacher(**filtered_inputs)\n",
    "                student_output = self.student(**filtered_inputs)\n",
    "                \n",
    "                loss_ce = student_output.loss\n",
    "                \n",
    "                logits_student = student_output.logits\n",
    "                logits_teacher = teacher_output.logits\n",
    "                \n",
    "                loss_kd = self.temperature ** 2 * self.criterion(\n",
    "                    F.log_softmax(logits_student / self.temperature, dim=-1),\n",
    "                    F.softmax(logits_teacher / self.temperature, dim=-1)\n",
    "                )\n",
    "                \n",
    "                loss = self.alpha * loss_ce + (1 - self.alpha) * loss_kd\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step_and_update_lr()  # Update this line\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                if i % 10 == 0:\n",
    "                    print(f'Epoch: {epoch}, Iteration: {i}, Loss: {train_loss / (i + 1)}')\n",
    "                    \n",
    "            val_loss = self.evaluate()\n",
    "            print(f'Epoch: {epoch}, Validation Loss: {val_loss}')\n",
    "            \n",
    "            # Save checkpoint after certain epochs\n",
    "            if epoch % 1 == 0:\n",
    "                torch.save(self.student.state_dict(), f'student_epoch_t_{epoch}.pth')\n",
    "                \n",
    "    def evaluate(self):\n",
    "        self.student.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(self.val_dataloader):\n",
    "                filtered_inputs = {\n",
    "                    'input_ids': batch['input_ids'].to(self.device),\n",
    "                    'attention_mask': batch['attention_mask'].to(self.device),\n",
    "                    'token_type_ids': batch.get('token_type_ids', torch.tensor([])).to(self.device) if 'token_type_ids' in batch else None,\n",
    "                    'labels': batch.get('labels', torch.tensor([])).to(self.device) if 'labels' in batch else None\n",
    "                }\n",
    "\n",
    "                filtered_inputs = {k: v for k, v in filtered_inputs.items() if v is not None}\n",
    "\n",
    "                student_output = self.student(**filtered_inputs)\n",
    "                teacher_output = self.teacher(**filtered_inputs)\n",
    "                \n",
    "                val_loss += student_output.loss.item()\n",
    "                \n",
    "                \n",
    "                \n",
    "        return val_loss / len(self.val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using trainer object\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([torch.tensor(item['input_ids']) for item in batch], batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence([torch.tensor(item['attention_mask']) for item in batch], batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Handle optional fields\n",
    "    token_type_ids = pad_sequence([torch.tensor(item['token_type_ids']) for item in batch], batch_first=True, padding_value=0) if 'token_type_ids' in batch[0] else None\n",
    "    labels = pad_sequence([torch.tensor(item['labels']) for item in batch], batch_first=True, padding_value=-100) if 'labels' in batch[0] else None\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Define the optimizer and scheduler\n",
    "optimizer = ScheduledOptim(torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9), 768, 2000)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the DataLoader with the custom collate_fn\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets['train'],\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    tokenized_datasets['validation'],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=batch_size, shuffle=True)\n",
    "# val_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the trainer\n",
    "trainer = DistillationTrainer(model_finetuned, model, train_dataloader, val_dataloader, optimizer, loss_fct, 1, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'token_type_ids', 'labels'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader)).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 16480,  7229,  1018, 21728,  3070,  1015,  1006, 22589,  1015,\n",
      "          1011,  1014,  1007,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  2194,  2036, 16360, 14326,  2070,  1002,  2538,  2454,\n",
      "          1997,  7016,  1010,  2092,  2682,  1996,  2761,  3740,  1002,  1022,\n",
      "          2454,  2000,  1002,  1023,  2454,  1012,   102,     0,     0,     0],\n",
      "        [  101,  2745,  8915, 10322,  4904,  2102,  1006,  2660,  1007,  5443,\n",
      "          1012,  4138,  3240, 10731,  4059,  1006,  1057,  1012,  1055,  1012,\n",
      "          1007,   102,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  5829,  2081, 10791,  1996,  2034,  2350,  2924,  2000,\n",
      "          3013,  6165,  1999,  3433,  2000,  1037,  2655,  2006,  5958,  2013,\n",
      "          2430,  2924,  3099,  2016,  2226, 11237,  1011, 11947,  1012,   102],\n",
      "        [  101, 12037,  1011,  2417, 12830,  2089,  6148,  3587,  4386,  2751,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2530,  2407,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  3145,  2093,  1011,  3204,  3446,  2001,  6706,  2012,\n",
      "          1017,  1012,  2871,  3867,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2012, 12755,  1011,  3565,  1011, 11941,  1024,  9296, 28188,\n",
      "          1006,  1040,  1012,  9574,  5345,  2025,  2041,  1010,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100,    3,    3,    0,    3,    3,    0,    0,    0,    0,    0,    0,\n",
      "            0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, -100, -100, -100, -100],\n",
      "        [-100,    1,    2,    2,    2,    2,    0,    5,    0,    0,    0,    1,\n",
      "            1,    2,    2,    0,    5,    5,    5,    5,    0, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    3,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
      "            1,    2,    2,    2,    0, -100],\n",
      "        [-100,    0,    0,    1,    1,    0,    0,    0,    7,    0,    0, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    7,    8, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    5,    5,    5,    5,    5,    0,    3,    0,    0,    1,\n",
      "            1,    2,    0,    0,    0,    0, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100]])}\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_dataloader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0, Loss: 37.29097366333008\n",
      "Epoch: 0, Iteration: 10, Loss: 43.02040758999911\n",
      "Epoch: 0, Iteration: 20, Loss: 42.79359890165783\n",
      "Epoch: 0, Iteration: 30, Loss: 44.08187349380985\n",
      "Epoch: 0, Iteration: 40, Loss: 42.46434355945122\n",
      "Epoch: 0, Iteration: 50, Loss: 39.87305659873813\n",
      "Epoch: 0, Iteration: 60, Loss: 36.84897607271789\n",
      "Epoch: 0, Iteration: 70, Loss: 34.42202940121503\n",
      "Epoch: 0, Iteration: 80, Loss: 32.68631500667996\n",
      "Epoch: 0, Iteration: 90, Loss: 30.96294330764603\n",
      "Epoch: 0, Iteration: 100, Loss: 29.408854007720947\n",
      "Epoch: 0, Iteration: 110, Loss: 28.13211576358692\n",
      "Epoch: 0, Iteration: 120, Loss: 27.123944199774876\n",
      "Epoch: 0, Iteration: 130, Loss: 26.169170346878868\n",
      "Epoch: 0, Iteration: 140, Loss: 25.32811566278444\n",
      "Epoch: 0, Iteration: 150, Loss: 24.587374557722484\n",
      "Epoch: 0, Iteration: 160, Loss: 23.884181529098417\n",
      "Epoch: 0, Iteration: 170, Loss: 23.35410077390615\n",
      "Epoch: 0, Iteration: 180, Loss: 22.791840919473554\n",
      "Epoch: 0, Iteration: 190, Loss: 22.36741368808047\n",
      "Epoch: 0, Iteration: 200, Loss: 21.920328171099005\n",
      "Epoch: 0, Iteration: 210, Loss: 21.419206682539663\n",
      "Epoch: 0, Iteration: 220, Loss: 21.06001916082736\n",
      "Epoch: 0, Iteration: 230, Loss: 20.676133692522587\n",
      "Epoch: 0, Iteration: 240, Loss: 20.317136337153645\n",
      "Epoch: 0, Iteration: 250, Loss: 19.969454377770898\n",
      "Epoch: 0, Iteration: 260, Loss: 19.698067018355445\n",
      "Epoch: 0, Iteration: 270, Loss: 19.377653157139175\n",
      "Epoch: 0, Iteration: 280, Loss: 19.167025888517657\n",
      "Epoch: 0, Iteration: 290, Loss: 18.8521624430758\n",
      "Epoch: 0, Iteration: 300, Loss: 18.641664368765696\n",
      "Epoch: 0, Iteration: 310, Loss: 18.36092570970296\n",
      "Epoch: 0, Iteration: 320, Loss: 18.157268002025805\n",
      "Epoch: 0, Iteration: 330, Loss: 17.934426426527363\n",
      "Epoch: 0, Iteration: 340, Loss: 17.732517177408393\n",
      "Epoch: 0, Iteration: 350, Loss: 17.596227349039495\n",
      "Epoch: 0, Iteration: 360, Loss: 17.389026985934567\n",
      "Epoch: 0, Iteration: 370, Loss: 17.26533780329311\n",
      "Epoch: 0, Iteration: 380, Loss: 17.06728123304412\n",
      "Epoch: 0, Iteration: 390, Loss: 16.908795071379913\n",
      "Epoch: 0, Iteration: 400, Loss: 16.79382647421592\n",
      "Epoch: 0, Iteration: 410, Loss: 16.630318663126072\n",
      "Epoch: 0, Iteration: 420, Loss: 16.468356537422487\n",
      "Epoch: 0, Iteration: 430, Loss: 16.325265082848322\n",
      "Epoch: 0, Iteration: 440, Loss: 16.195477580267287\n",
      "Epoch: 0, Iteration: 450, Loss: 16.05992637981068\n",
      "Epoch: 0, Iteration: 460, Loss: 15.987322934537545\n",
      "Epoch: 0, Iteration: 470, Loss: 15.853137080077152\n",
      "Epoch: 0, Iteration: 480, Loss: 15.759673607324611\n",
      "Epoch: 0, Iteration: 490, Loss: 15.615727869410621\n",
      "Epoch: 0, Iteration: 500, Loss: 15.554995936547925\n",
      "Epoch: 0, Iteration: 510, Loss: 15.458537762412353\n",
      "Epoch: 0, Iteration: 520, Loss: 15.345296154553052\n",
      "Epoch: 0, Iteration: 530, Loss: 15.250110843985318\n",
      "Epoch: 0, Iteration: 540, Loss: 15.149070435222548\n",
      "Epoch: 0, Iteration: 550, Loss: 15.062308690076298\n",
      "Epoch: 0, Iteration: 560, Loss: 14.99901427007189\n",
      "Epoch: 0, Iteration: 570, Loss: 14.871000417268423\n",
      "Epoch: 0, Iteration: 580, Loss: 14.791990602283182\n",
      "Epoch: 0, Iteration: 590, Loss: 14.709468162604395\n",
      "Epoch: 0, Iteration: 600, Loss: 14.638968086480698\n",
      "Epoch: 0, Iteration: 610, Loss: 14.589448158947809\n",
      "Epoch: 0, Iteration: 620, Loss: 14.516305400169607\n",
      "Epoch: 0, Iteration: 630, Loss: 14.432081330219274\n",
      "Epoch: 0, Iteration: 640, Loss: 14.357813558042887\n",
      "Epoch: 0, Iteration: 650, Loss: 14.278565796472693\n",
      "Epoch: 0, Iteration: 660, Loss: 14.195138631177201\n",
      "Epoch: 0, Iteration: 670, Loss: 14.14873859033144\n",
      "Epoch: 0, Iteration: 680, Loss: 14.100272967076688\n",
      "Epoch: 0, Iteration: 690, Loss: 14.005127424441612\n",
      "Epoch: 0, Iteration: 700, Loss: 13.928692840135387\n",
      "Epoch: 0, Iteration: 710, Loss: 13.875630630219536\n",
      "Epoch: 0, Iteration: 720, Loss: 13.82402034738358\n",
      "Epoch: 0, Iteration: 730, Loss: 13.778328154546944\n",
      "Epoch: 0, Iteration: 740, Loss: 13.717358161241581\n",
      "Epoch: 0, Iteration: 750, Loss: 13.656092857076388\n",
      "Epoch: 0, Iteration: 760, Loss: 13.593635998950212\n",
      "Epoch: 0, Iteration: 770, Loss: 13.53626569235835\n",
      "Epoch: 0, Iteration: 780, Loss: 13.451226997314434\n",
      "Epoch: 0, Iteration: 790, Loss: 13.390396650160007\n",
      "Epoch: 0, Iteration: 800, Loss: 13.333092203449816\n",
      "Epoch: 0, Iteration: 810, Loss: 13.254493538166239\n",
      "Epoch: 0, Iteration: 820, Loss: 13.211346852213108\n",
      "Epoch: 0, Iteration: 830, Loss: 13.138801181072505\n",
      "Epoch: 0, Iteration: 840, Loss: 13.078580842715523\n",
      "Epoch: 0, Iteration: 850, Loss: 13.039512447408727\n",
      "Epoch: 0, Iteration: 860, Loss: 12.982322993871088\n",
      "Epoch: 0, Iteration: 870, Loss: 12.954028125471691\n",
      "Epoch: 0, Iteration: 880, Loss: 12.911287252261609\n",
      "Epoch: 0, Iteration: 890, Loss: 12.855665275143453\n",
      "Epoch: 0, Iteration: 900, Loss: 12.811657112525914\n",
      "Epoch: 0, Iteration: 910, Loss: 12.763383477762686\n",
      "Epoch: 0, Iteration: 920, Loss: 12.735894040875015\n",
      "Epoch: 0, Iteration: 930, Loss: 12.6832140260301\n",
      "Epoch: 0, Iteration: 940, Loss: 12.633804487244609\n",
      "Epoch: 0, Iteration: 950, Loss: 12.581572569256451\n",
      "Epoch: 0, Iteration: 960, Loss: 12.530919658529895\n",
      "Epoch: 0, Iteration: 970, Loss: 12.484646434027674\n",
      "Epoch: 0, Iteration: 980, Loss: 12.438033177582861\n",
      "Epoch: 0, Iteration: 990, Loss: 12.393467447230842\n",
      "Epoch: 0, Iteration: 1000, Loss: 12.350488413344848\n",
      "Epoch: 0, Iteration: 1010, Loss: 12.292498889002436\n",
      "Epoch: 0, Iteration: 1020, Loss: 12.260554532930035\n",
      "Epoch: 0, Iteration: 1030, Loss: 12.200086512574863\n",
      "Epoch: 0, Iteration: 1040, Loss: 12.183737592555145\n",
      "Epoch: 0, Iteration: 1050, Loss: 12.1509709692818\n",
      "Epoch: 0, Iteration: 1060, Loss: 12.113770058310111\n",
      "Epoch: 0, Iteration: 1070, Loss: 12.071204620694358\n",
      "Epoch: 0, Iteration: 1080, Loss: 12.03503979586761\n",
      "Epoch: 0, Iteration: 1090, Loss: 11.998023126236786\n",
      "Epoch: 0, Iteration: 1100, Loss: 11.949733759899988\n",
      "Epoch: 0, Iteration: 1110, Loss: 11.914453059676314\n",
      "Epoch: 0, Iteration: 1120, Loss: 11.899916238384945\n",
      "Epoch: 0, Iteration: 1130, Loss: 11.867191761178743\n",
      "Epoch: 0, Iteration: 1140, Loss: 11.841296443596521\n",
      "Epoch: 0, Iteration: 1150, Loss: 11.811864308747491\n",
      "Epoch: 0, Iteration: 1160, Loss: 11.77885358134392\n",
      "Epoch: 0, Iteration: 1170, Loss: 11.73703583746665\n",
      "Epoch: 0, Iteration: 1180, Loss: 11.699991170679684\n",
      "Epoch: 0, Iteration: 1190, Loss: 11.664710528504237\n",
      "Epoch: 0, Iteration: 1200, Loss: 11.634924428250569\n",
      "Epoch: 0, Iteration: 1210, Loss: 11.613587278260562\n",
      "Epoch: 0, Iteration: 1220, Loss: 11.57957766311858\n",
      "Epoch: 0, Iteration: 1230, Loss: 11.541894942546259\n",
      "Epoch: 0, Iteration: 1240, Loss: 11.553443902351509\n",
      "Epoch: 0, Iteration: 1250, Loss: 11.53324916475206\n",
      "Epoch: 0, Iteration: 1260, Loss: 11.498822510951479\n",
      "Epoch: 0, Iteration: 1270, Loss: 11.461828615982451\n",
      "Epoch: 0, Iteration: 1280, Loss: 11.438396582651846\n",
      "Epoch: 0, Iteration: 1290, Loss: 11.444046635853054\n",
      "Epoch: 0, Iteration: 1300, Loss: 11.42038001874884\n",
      "Epoch: 0, Iteration: 1310, Loss: 11.405581155230669\n",
      "Epoch: 0, Iteration: 1320, Loss: 11.392823925621125\n",
      "Epoch: 0, Iteration: 1330, Loss: 11.36232343022757\n",
      "Epoch: 0, Iteration: 1340, Loss: 11.338035813291778\n",
      "Epoch: 0, Iteration: 1350, Loss: 11.32081612624741\n",
      "Epoch: 0, Iteration: 1360, Loss: 11.28326171448264\n",
      "Epoch: 0, Iteration: 1370, Loss: 11.269355276568279\n",
      "Epoch: 0, Iteration: 1380, Loss: 11.252245382672198\n",
      "Epoch: 0, Iteration: 1390, Loss: 11.229310094324656\n",
      "Epoch: 0, Iteration: 1400, Loss: 11.192004655957817\n",
      "Epoch: 0, Iteration: 1410, Loss: 11.157936244886196\n",
      "Epoch: 0, Iteration: 1420, Loss: 11.123438668368486\n",
      "Epoch: 0, Iteration: 1430, Loss: 11.101334772394887\n",
      "Epoch: 0, Iteration: 1440, Loss: 11.078719334954773\n",
      "Epoch: 0, Iteration: 1450, Loss: 11.057770035621793\n",
      "Epoch: 0, Iteration: 1460, Loss: 11.037575514123997\n",
      "Epoch: 0, Iteration: 1470, Loss: 11.010025039041714\n",
      "Epoch: 0, Iteration: 1480, Loss: 10.996772116579614\n",
      "Epoch: 0, Iteration: 1490, Loss: 10.984820379537675\n",
      "Epoch: 0, Iteration: 1500, Loss: 10.967563232090535\n",
      "Epoch: 0, Iteration: 1510, Loss: 10.951909602398434\n",
      "Epoch: 0, Iteration: 1520, Loss: 10.93534269123623\n",
      "Epoch: 0, Iteration: 1530, Loss: 10.90798139155573\n",
      "Epoch: 0, Iteration: 1540, Loss: 10.88664344024689\n",
      "Epoch: 0, Iteration: 1550, Loss: 10.862700682659444\n",
      "Epoch: 0, Iteration: 1560, Loss: 10.855462996987178\n",
      "Epoch: 0, Iteration: 1570, Loss: 10.82869917935888\n",
      "Epoch: 0, Iteration: 1580, Loss: 10.814487142136095\n",
      "Epoch: 0, Iteration: 1590, Loss: 10.79542685577811\n",
      "Epoch: 0, Iteration: 1600, Loss: 10.78239962255709\n",
      "Epoch: 0, Iteration: 1610, Loss: 10.764025864025438\n",
      "Epoch: 0, Iteration: 1620, Loss: 10.743839394855028\n",
      "Epoch: 0, Iteration: 1630, Loss: 10.729050076460998\n",
      "Epoch: 0, Iteration: 1640, Loss: 10.704486302418509\n",
      "Epoch: 0, Iteration: 1650, Loss: 10.67484631706627\n",
      "Epoch: 0, Iteration: 1660, Loss: 10.65997788250841\n",
      "Epoch: 0, Iteration: 1670, Loss: 10.646992356137984\n",
      "Epoch: 0, Iteration: 1680, Loss: 10.642992111595523\n",
      "Epoch: 0, Iteration: 1690, Loss: 10.633934363925478\n",
      "Epoch: 0, Iteration: 1700, Loss: 10.612426301193967\n",
      "Epoch: 0, Iteration: 1710, Loss: 10.59391134656848\n",
      "Epoch: 0, Iteration: 1720, Loss: 10.581780102868056\n",
      "Epoch: 0, Iteration: 1730, Loss: 10.568073244090717\n",
      "Epoch: 0, Iteration: 1740, Loss: 10.558034030854325\n",
      "Epoch: 0, Iteration: 1750, Loss: 10.537429760313932\n",
      "Epoch: 0, Validation Loss: 0.4504294698595708\n"
     ]
    }
   ],
   "source": [
    "trainer.train(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
